Can you review this project plan to implement a leet code trainer on an existing site already providing auth, express backend and supabase database.
Edit the plan in place and return just an updated plan without any revised comments or suggestions but the new plan fully updated

---

### **Project Plan**

### **Existing Items**
* JWT auth, CORS and rate limiting are already handled
* Existing OpenRouter API integration with list of free models as options

Here is the optimized plan incorporating these fixes.

#### **1. Project Overview **
**Objective:** Build a web app that generates LeetCode-style problems via OpenRouter.
**Key Architectural Change:** Decouple **Generated Problems** (static) from **User Attempts** (dynamic) to allow problem re-use and better data integrity.

#### **2. Technical Stack **
*   **Frontend:** React.js mui Tailwind CSS
*   **Backend:** Node.js (Express)
*   **Database:** PostgreSQL (Supabase)
*   **AI:** OpenRouter (Target `deepseek-coder` or `mistral-7b` for JSON reliability)
*   **Auth:** JWT

#### **3. Project Phases (Optimized)**

**Phase 1: Architecture & Setup (1 Day)**
*   **DB Schema Design (Refined):**
    *   Table 1: `problems` (id, focus_area, problem_json, created_at)
    *   Table 2: `attempts` (id, user_id, problem_id, user_solution, rating)
*   **Prompt Engineering:** Create the prompt template.
*   **Boilerplate:** Initialize React + Express repos.

**Phase 2: Backend Development **
*   **AI Service Layer:**
    *   Call OpenRouter.
    *   **Crucial Step:** Implement a "sanitizer" function to strip markdown wrappers (```json) from the AI response before parsing.
    *   **Prompt & Output Parsing** | â€¢ Use **strict JSON mode** in the OpenRouter request (if supported) and set `strict: true` in the request payload. 
*   **API Endpoints:**
    *   `GET /problems/generate`:
        *   Checks DB first: "Is there an unused problem for this focus area?"
        *   Only show results for 'active' problems
        *   If no: Call AI -> Save to `problems` -> Return.
        *   **Important!** use existing openrouter_events table and logging to save OpenRouter API calls with the result parsed and saved to problems table
    *   `POST /problems/attempt`: Save user solution/rating to `user_attempts`.
    *   `GET /progress`: Join tables to show user history.

**Phase 3: Frontend Development **
*   **Code Execution** 
        **Sandpack** https://github.com/codesandbox/sandpack
        - Sandpack is a component toolkit for creating your own live running code editing experience powered by CodeSandbox.
*   **UI Layout:**
    *   **Sidebar:** Focus areas (buttons) + Progress Bar (visual percentage).
    *   **Main Area:**
        *   State A: "Select a topic" (Empty state).
        *   State B: "Loading..." (While AI generates).
        *   State C: Problem Display (Description, Test Cases).
        *   State D: Editor (Simple textarea) + Run/Submit buttons.
        *   State E: All test cases pass the user submitted solution (Gong sound)
*   **State Management:**
    *   Use Redux Toolkit

**Phase 4: AI Integration & Refinement **
*   **Handling Hallucinations:**
    *   The AI might generate "Easy" difficulty when you asked for "Hard."
    *   *Fix:* Add a post-processing check in the backend to validate difficulty tags.
*   **Rate Limiting:** Prevent users from spamming the "Generate" button (and costing you money).

**Phase 5: Testing & Deployment **
*   **Testing:**
    *   Test "Bad JSON" response: Mock an AI response with markdown and ensure your backend handles it.
    *   Test "Empty Input": Ensure user can't submit blank code.
*   **Deployment:**
    *   **Frontend:** Vercel.
    *   **Backend/DB:** Supabase (handles Postgres + Auth in one go) OR Railway/Render.

#### **4. Revised Timeline**
*   **Day 1:** Setup DB, Auth, and Backend Boilerplate.
*   **Day 2-3:** Build AI integration with Zod validation and sanitizers.
*   **Day 4:** Build Frontend Skeleton and Auth UI.
*   **Day 5-7:** Connect Frontend to Backend (Generate & Save flow).
*   **Day 8:** Polish UI (Tailwind), add Editor.
*   **Day 9:** Testing (Edge cases).
*   **Day 10:** Deploy.

---

### **Appendix A: Prompt Template**
*Use this to reduce JSON parsing errors.*

```json
{
  "role": "system",
  "content": "You are a strict JSON generator. You must output ONLY valid JSON. No markdown, no explanation. The JSON must match this schema: { problem: string, test_cases: [{input: string, output: string}], solution_code: string, starter_code: string, difficulty: 'easy'|'medium'|'hard' }"
}
```

### **Appendix B: PostgreSQL Schema**
*Normalized for efficiency.*


```sql
-- Stores the raw AI-generated content
CREATE TABLE problems (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    model character varying not null, -- The OpenRouter AI model used
    generation_id character varying null, -- OpenRouter response id
    focus_area VARCHAR(100) NOT NULL,
    raw_content JSONB NOT NULL, -- Stores the full AI JSON response
    active boolean null default true,
    created_at TIMESTAMP DEFAULT NOW(),
    created_by UUID NULL -- If you want to track who generated it (optional)
);

-- Stores user interactions
CREATE TABLE attempts (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL, -- References your auth table
    problem_id UUID REFERENCES problems(id),
    user_solution TEXT,
    rating INT CHECK (rating BETWEEN 1 AND 5),
    created_at TIMESTAMP DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_attempts_user ON attempts(user_id);
CREATE INDEX idx_problems_area ON problems(focus_area);

- Composite index on `(focus_area, created_at)` for finding unused problems
- Partial index for problems without attempts yet

```
